---
layout: 	landing-page
year:		2023
title: 		'@CVPR 2023'
date: 		'Date: June 19th'
location: 	'Location: Vancouver, BC, Canada'
banner:		/images/2023/banner.jpg

redirect_from: "/"
---
<!-- Main -->
<article id="main">


        <!-- Three -->
    <section class="wrapper style3 container special">
	<!-- Content -->
        <div class="content">
           <section>
                <header>
                    <h2><strong>Dynavis 2021 ReCap</strong></h2>
                </header>
                <a href="https://www.youtube.com/watch?v=SVSNU6vKwwY" target="_blank"><img src="/images/2021/yt-preview.png" alt="" style="width:100%;"/></a>
                
           	</section>
        </div>
    </section>

<!-- One -->
    <section class="wrapper style3 container">

        <!-- Content -->
        <div class="content">
            <section>
                <header>
                    <h2>The 4th International Workshop on Dynamic Scene Reconstruction</h2>
                </header>
                <p>
                    Reconstruction of general dynamic scenes is motivated by potential applications in film and broadcast production together with the ultimate goal of automatic understanding of real-world scenes from distributed camera networks. With recent advances in hardware and the advent of virtual and augmented reality, dynamic scene reconstruction is being applied to more complex scenes with applications in Entertainment, Games, Film, Creative Industries and AR/VR/MR. We welcome contributions to this workshop in the form of oral presentations and posters. Suggested topics include, but are not limited to:
                </p>
                <ul>
                    <li>- Dynamic 3D reconstruction from single, stereo or multiple views</li>
                    <li>- Learning-based methods in dynamic scene reconstruction and understanding</li>
                    <li>- Multi-modal dynamic scene modelling (RGBD, LIDAR, 360 video, light fields)</li>
                    <li>- 4D reconstruction and modelling</li>
                    <li>- 3D/4D data acquisition, representation, compression and transmission</li>
                    <li>- Scene analysis and understanding in 2D and 3D</li>
                    <li>- Structure from motion, camera calibration and pose estimation</li>
                    <li>- Digital humans: motion and performance capture, bodies, faces, hands</li>
                    <li>- Geometry processing</li>
                    <li>- Computational photography</li>
                    <li>- Appearance and reflectance modelling</li>
                    <li>- Scene modelling in the wild, moving cameras, handheld cameras</li>
                    <li>- Applications of dynamic scene reconstruction (VR/AR, character animation, free-viewpoint video, relighting, medical imaging, creative content production, animal tracking, HCI, sports)</li>
                </ul>
               
                <p><strong>The objectives for this workshop are to:</strong></p>
                <ul>
                    <li>- Bringing together leading experts in the field of general dynamic scene reconstruction to help propel the field forward.</li>
                    <li>- Create and maintain an online database of datasets and papers</li>
                    <li>- Accelerate research progress in the field of dynamic scene reconstruction to match the requirements of real-world applications by identifying the challenges and ways to address them through a panel discussion between experts, presenters and attendees.</li>
                </ul>

            </section>
        </div>
    </section>
    
    
   
    
    
    


    
        
    <!-- Three -->
    <section class="wrapper style3 container special" id="speakers">
    
        <header class="major">
            <h2><strong>Speakers</strong></h2>
        </header>
	
    <div class="row">
        <div class="3u">
            <a href="https://mysite.ku.edu.tr/fguney/" target="_blank">
                <section>
                    <img class="image-circle" src="/images/2023/fg.jpeg" alt=""/>
                    <header>
                        <h3><strong>Fatma <br> G端ney</strong></h3>
                    </header>
                    <!--<p>Assistant Professor at Koc University</p>-->
                </section>
            </a>
         </div>
         <div class="7u">
            <p><strong>Fatma G端ney</strong> is an Assistant Professor at Koc University in Istanbul. She received her Ph.D. from MPI in Germany and worked as a postdoctoral researcher at VGG in Oxford. She is a recipient of multiple outstanding reviewer awards at CVPR and ICCV. Her research interests include 3D computer vision and representation learning from video sequences. </p>
        </div>
    </div>
    <div class="row">
        <div class="3u">
            <a href="https://virtualhumans.mpi-inf.mpg.de/people/pons-moll.html" target="_blank">
                <section>
                    <img class="image-circle" src="/images/2023/gp.jpg" alt=""/>
                    <header>
                        <h3><strong>Gerard <br> Pons-Moll</strong></h3>
                    </header>
                    <!--<p>Assistant Professor at Koc University</p>-->
                </section>
            </a>
         </div>
        <div class="7u">
            <p><strong>Gerard Pons-Moll</strong> is a Professor at the University of T端bingen, at the department of Computer Science. He is also the head of the Emmy Noether independent research group "Real Virtual Humans", senior researcher at the Max Planck for Informatics (MPII) in Saarbr端cken, Germany. His research lies at the intersection of computer vision, computer graphics and machine learning. His research has produced some of the most advanced statistical human body models of pose, shape, soft-tissue and clothing (which are currently used for a number of applications in industry and research), as well as algorithms to track and reconstruct 3D people models from images, video, depth, and IMUs. His most recent interests span human-scene interaction, and 3D scene and object representation learning.</p>
                <br>
                <p><strong>Capturing and modelling human behavior with neural fields</strong>
                <br>
                <strong>Abstract:</strong> Capturing and modelling 3D humans and the objects they are interacting with from consumer grade sensors like RGB and RGBD cameras is extremely challenging due to heavy occlusions, noise and complex interaction.
                Clearly solving this problem requires learned models of the human shape, pose and of the space of possible interactions. In this talk, I will show that neural fields are a power paradigm to learn such models from data, and I will show how they can be combined with classical model based fitting techniques to obtain the best of both worlds. I will also show that joint reasoning about humans and objects/scene is crucial for capture and animation, and I will show how object pose can even be inferred based on human pose alone.
                </p>
                
            
        </div>
    </div>
	
    </section>
    
    
    
    
    <!-- Three -->
    <section class="wrapper style3 container special" id="program">
    
        <header class="major">
            <h2><strong>Program</strong></h2>
        </header>
           
       <div class="row">
           <table >
                <colgroup>
                   <col span="1" style="width: 25%;">
                   <col span="1" style="width: 75%;">
                  </colgroup>
                    <tbody >
                        <tr>
                            <td><h3>Time (PT)</h3></td>
                            <td><h3>Session</h3></td>
                        </tr>
                        <tr>
                            <td><span >13:30 - 13:40</span></td>
                            <td><span ><b>Welcome and Introduction</b></span><br></td>
                        </tr>
                        <br>
                        <tr>
                            <td><span >13:40 - 14:25</span></td>
                            <td><span ><b>Keynote 1: Fatma Guney</span><br></td>
                        </tr>
                        <tr>
                            <td><span >14:25 - 15:10</span></td>
                            <td>
                                <span ><b>Paper Session (15 mins each)</b></span><br/>
                                
                                <p><b>CAMM: Building Category-Agnostic and Animatable 3D Models from Monocular Videos</b><br>
                                T. Kuai, A. Karthikeyan, Y.M. Kant, A. Mirzaei and I. Gilitschenski<br>
                            
                                <p><b>Unbiased 4D: Monocular 4D Reconstruction with a Neural Deformation Model</b><br>
                                E. CM Johnson, M. Habermann, S. Shimada, V. Golyanik and C. Theobalt<br>
         
                                <p><b>Robust Monocular 3D Human Motion with Lasso-Based Differential Kinematics</b><br>
                                A. Malti </p><br>
                            </td>
                        </tr>
                        <tr>
                            <td><span >15:10 - 15:45</span></td>
                            <td><span ><b>Coffee Break</b></span></td>
                        </tr>
                        <tr>
                            <td><span >15:45 - 16:30</span></td>
                            <td><span ><b>Keynote 2: Gerard Pons Moll</b></span><br>
                            </td>
                        </tr>
                        <tr>
                            <td><span >16:30 - 17.15</span></td>
                            <td><span ><b>Paper Session 2 (15 mins each)</b></span><br>
                                
                                <p><b>CAT-NeRF: Constancy-Aware Tx^2Former for Dynamic Body Modeling</b><br>
                                H. Zhu, Z. Zheng, W. Zheng and R. Nevatia<br>
                            
                                <p><b>DynamicStereo: Consistent Dynamic Depth from Stereo Videos</b><br>
                                Nikita Karaev, Ignacio Rocco, Benjamin Graham, Natalia Neverova, Andrea Vedaldi, Christian Rupprecht<br>
                                <b style="color:red">Invited from CVPR 2023 </b></p><br>
                                <br>
                            
                                <p><b>DynIBaR Neural Dynamic Image-Based Rendering</b><br>
                                Zhengqi Li, Qianqian Wang, Forrester Cole, Richard Tucker, Noah Snavely <br>
                                <b style="color:red">Invited from CVPR 2023 (Award Candiate)</b></p><br>
    
    


                            </td>
                        </tr>
                        <tr>
                            <td><span >17:15 - 17:20</span></td>
                            <td><span ><b>Closing Remarks</b></span><br>
                            </td>
                        </tr>
                </tbody>
            </table>
        </div>
       
   </section>

   

     
    <!-- Three -->
    <section class="wrapper style3 container special" id="organizers">
    
        <header class="major">
            <h2><strong>Organizers</strong></h2>
        </header>
        <div class="row">
            <div class="3u">
                <a href="https://arminmustafa.github.io/" target="_blank">
                    <section>
                        <img class="image-circle" src="/images/organisers/am.jpg" alt=""/>
                        <header>
                            <h3><strong>Armin<br> Mustafa</strong></h3>
                        </header>
                        <p>Centre for Vision, Speech and Signal Processing, University of Surrey, UK</p>
                    </section>
                </a>
            </div>
            <div class="3u">
                <a href="https://marcovolino.github.io/" target="_blank">
                    <section>
                        <img class="image-circle" src="/images/organisers/mv.png" alt=""/>
                        <header>
                            <h3><strong>Marco<br> Volino</strong></h3>
                        </header>
                        <p>Centre for Vision, Speech and Signal Processing, University of Surrey, UK</p>
                    </section>
                </a>
            </div>
            <div class="3u">
                <a href="https://dancasas.github.io/" target="_blank">
                    <section>
                        <img class="image-circle" src="/images/organisers/dc.jpg" alt=""/>
                        <header>
                            <h3><strong>Dan<br> Casas</strong></h3>
                        </header>
                        <p>Multimodal Simulation Lab, Universidad Rey Juan Carlos, Spain</p>
                    </section>
                </a>
            </div>
        </div>

        <div class="row">
            <div class="3u">
                <a href="https://richardt.name/" target="_blank">
                    <section>
                        <img class="image-circle" src="/images/organisers/cr.jpg" alt=""/>
                        <header>
                            <h3><strong>Christian<br> Richardt</strong></h3>
                        </header>
                        <p>Meta Reality Labs Research, US</p>
                    </section>
                </a>
            </div>
            <div class="3u">
                <a href="https://www.surrey.ac.uk/people/adrian-hilton" target="_blank">
                    <section>
                        <img class="image-circle" src="/images/organisers/ah.png" alt=""/>
                        <header>
                            <h3><strong>Adrian<br> Hilton</strong></h3>
                        </header>
                        <p>Centre for Vision, Speech and Signal Processing, University of Surrey, UK</p>
                    </section>
                </a>
            </div>
        </div>
    </section>
    

    <!-- Three -->
    <section class="wrapper style3 container special" id="programcomittee">
    
        <header class="major">
            <h2><strong>Program Committee</strong></h2>
        </header>
        
        <p>The organising committee would like to express their gratitude to the program committee who provided their time and expertise to ensure accepted papers are high quality, suitable for DynaVis and of sound scientific merit.</p>
        
        <div class="row">
            
            <div class="4u">
                <h5><strong>Akin Caliskan</strong><br>Flawless AI</h5>
            </div>
            <div class="4u">
                <h5><strong>Fabian Prada</strong><br>Meta</h5>
            </div>
            <div class="4u">
                <h5><strong>Franziska Mueller</strong><br>Google</h5>
            </div>
            
            <div class="4u">
                <h5><strong>Helge Rhodin</strong><br>UBC</h5>
            </div>
            <div class="4u">
                <h5><strong>Marco Pesavento</strong><br>University of Surrey</h5>
            </div>
          <div class="4u">
              <h5><strong>Timur Bagautdinov</strong><br>Meta Reality Labs</h5>
          </div>
  
        </div>
    </section>
    
    
    <!-- Three -->
    <section class="wrapper style3 container special" id="submission">
    
        <header class="major">
            <h2><strong>Submission</strong></h2>
       </header>
         
        <p>We welcome submissions from both industry and academia, including interdisciplinary work and work from those outside of the mainstream computer vision community.</p>
                 
        <div class="row">
  
              <div class="6u">
            
                <section>
                    <header>
                        <h3>Instructions</h3>
                    </header>
                    <p>Papers will be limited up to 8 pages according to the <a href="https://cvpr.thecvf.com/Conferences/2023/AuthorGuidelines" target="_blank">CVPR format</a> (main conference authors guidelines). All papers will be reviewed with double blind policy. Papers will be selected based on relevance, significance and novelty of results, technical merit, and clarity of presentation.</p>
                    <p><a href="https://cmt3.research.microsoft.com/DynaVis2023" target="_blank">Submission website</a> </p>
                </section>
            </div>
            <div class="6u">
            
                <section>
                    <header>
                        <h3>Important Dates</h3>
                    </header>
                        <table >
                            <tbody >
                                <tr>
                                    <td><strong>Action</strong></td>
                                    <td><strong>Date</strong></td>
                                </tr>
                                <tr>
                                    <td><span >Paper submission deadline</span></td>
                                    <td><span ><s>March 10<b style="color:red;">17</b>  2023</s></span></td>
                                </tr>
                                <tr>
                                    <td><span >Notification to authors</span></td>
                                    <td><span ><span ><s>March 29, 2023</s></span></td>
                                </tr>
                                <tr>
                                    <td><span >Camera ready deadline</span></td>
                                    <td><span ><span ><s>April 5, 2023</s></span></td>
                                </tr>
                            </tbody>
                        </table>
                </section>
            </div>
       </section>
  
  
</article>
